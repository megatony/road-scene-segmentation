{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf9b4938-1974-4ff1-baef-581bffd3e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔵 ISTANBUL için dataset oluşturuluyor...\n",
      "🧹 /truba/home/baakgul/roadtr-14032025-istanbul siliniyor...\n",
      "✅ train klasörü symlink yapıldı.\n",
      "✅ valid klasörü istanbul için hazır.\n",
      "✅ test klasörü istanbul için hazır.\n",
      "✅ data.yaml istanbul için kaydedildi.\n",
      "\n",
      "🔵 PARIS için dataset oluşturuluyor...\n",
      "🧹 /truba/home/baakgul/roadtr-14032025-paris siliniyor...\n",
      "✅ train klasörü symlink yapıldı.\n",
      "✅ valid klasörü paris için hazır.\n",
      "✅ test klasörü paris için hazır.\n",
      "✅ data.yaml paris için kaydedildi.\n",
      "\n",
      "🔵 MUNIH için dataset oluşturuluyor...\n",
      "🧹 /truba/home/baakgul/roadtr-14032025-munih siliniyor...\n",
      "✅ train klasörü symlink yapıldı.\n",
      "✅ valid klasörü munih için hazır.\n",
      "✅ test klasörü munih için hazır.\n",
      "✅ data.yaml munih için kaydedildi.\n",
      "\n",
      "🔵 MARSILYA için dataset oluşturuluyor...\n",
      "🧹 /truba/home/baakgul/roadtr-14032025-marsilya siliniyor...\n",
      "✅ train klasörü symlink yapıldı.\n",
      "✅ valid klasörü marsilya için hazır.\n",
      "✅ test klasörü marsilya için hazır.\n",
      "✅ data.yaml marsilya için kaydedildi.\n",
      "\n",
      "\n",
      "🏁 Tüm şehirler için datasetler başarıyla oluşturuldu!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "README: City-Specific YOLO Dataset Splitter\n",
    "\n",
    "This script creates separate datasets for each city in a multi-city YOLO dataset.\n",
    "- For each city, creates a new dataset directory with symlinks to the original 'train' images (full),\n",
    "  and filtered symlinks for 'valid' and 'test' splits based on city-specific filename keywords.\n",
    "- Updates and writes a new data.yaml for each city with correct split paths.\n",
    "\n",
    "How to use:\n",
    "- Set 'original_dataset' and 'target_base' paths as needed.\n",
    "- Define your 'city_keywords' according to your dataset's city naming conventions.\n",
    "- Run this script; one subfolder per city will be created, each with a ready-to-use data.yaml.\n",
    "\n",
    "Author: Bahadir Akin Akgul\n",
    "Date: 13.07.2025\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Original dataset root\n",
    "original_dataset = Path('/PATH/TO/original-dataset-root')\n",
    "\n",
    "# Splits and data.yaml\n",
    "train_dir = original_dataset / 'train'\n",
    "valid_dir = original_dataset / 'valid'\n",
    "test_dir = original_dataset / 'test'\n",
    "data_yaml_path = original_dataset / 'data.yaml'\n",
    "\n",
    "# City keyword filter (edit as needed)\n",
    "city_keywords = {\n",
    "    'istanbul': ['libadiye', 'levent', 'taksim', 'ciragan', 'barbaros', 'dolmabahce', 'bagdat', 'muallim', 'katar'],\n",
    "    'paris': ['paris-champs'],\n",
    "    'munich': ['munih'],\n",
    "    'marseille': ['marsilya']\n",
    "}\n",
    "\n",
    "# Output base directory\n",
    "target_base = Path('/PATH/TO/output-dataset-root')\n",
    "\n",
    "for city, keywords in city_keywords.items():\n",
    "    print(f\"Creating dataset for {city.upper()}...\")\n",
    "\n",
    "    # City-specific dataset path\n",
    "    city_dataset_dir = target_base / f'dataset-city-{city}'\n",
    "\n",
    "    # Remove old city dataset if exists\n",
    "    if city_dataset_dir.exists():\n",
    "        print(f\"Removing old directory: {city_dataset_dir}\")\n",
    "        shutil.rmtree(city_dataset_dir)\n",
    "\n",
    "    # Symlink full train/ directory\n",
    "    src_train = train_dir\n",
    "    dst_train = city_dataset_dir / 'train'\n",
    "    dst_train.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if not dst_train.exists():\n",
    "        os.symlink(src_train, dst_train)\n",
    "    print(f\"Train folder symlinked.\")\n",
    "\n",
    "    # Filtered valid/ symlinks\n",
    "    city_valid_images_dir = city_dataset_dir / 'valid' / 'images'\n",
    "    city_valid_labels_dir = city_dataset_dir / 'valid' / 'labels'\n",
    "    city_valid_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    city_valid_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img_path in (valid_dir / 'images').glob('*.*'):\n",
    "        img_name = img_path.name.lower()\n",
    "        if any(keyword in img_name for keyword in keywords):\n",
    "            dst_img = city_valid_images_dir / img_path.name\n",
    "            if not dst_img.exists():\n",
    "                os.symlink(img_path, dst_img)\n",
    "\n",
    "            label_path = valid_dir / 'labels' / (img_path.stem + '.txt')\n",
    "            if label_path.exists():\n",
    "                dst_lbl = city_valid_labels_dir / label_path.name\n",
    "                if not dst_lbl.exists():\n",
    "                    os.symlink(label_path, dst_lbl)\n",
    "            else:\n",
    "                print(f\"Label not found (valid): {label_path}\")\n",
    "\n",
    "    print(f\"Valid folder for {city} ready.\")\n",
    "\n",
    "    # Filtered test/ symlinks\n",
    "    city_test_images_dir = city_dataset_dir / 'test' / 'images'\n",
    "    city_test_labels_dir = city_dataset_dir / 'test' / 'labels'\n",
    "    city_test_images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    city_test_labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for img_path in (test_dir / 'images').glob('*.*'):\n",
    "        img_name = img_path.name.lower()\n",
    "        if any(keyword in img_name for keyword in keywords):\n",
    "            dst_img = city_test_images_dir / img_path.name\n",
    "            if not dst_img.exists():\n",
    "                os.symlink(img_path, dst_img)\n",
    "\n",
    "            label_path = test_dir / 'labels' / (img_path.stem + '.txt')\n",
    "            if label_path.exists():\n",
    "                dst_lbl = city_test_labels_dir / label_path.name\n",
    "                if not dst_lbl.exists():\n",
    "                    os.symlink(label_path, dst_lbl)\n",
    "            else:\n",
    "                print(f\"Label not found (test): {label_path}\")\n",
    "\n",
    "    print(f\"Test folder for {city} ready.\")\n",
    "\n",
    "    # Load and update data.yaml\n",
    "    with open(data_yaml_path, 'r') as f:\n",
    "        data_yaml = yaml.safe_load(f)\n",
    "\n",
    "    # Update paths for the city's split\n",
    "    data_yaml['train'] = str(dst_train / 'images')\n",
    "    data_yaml['val'] = str(city_dataset_dir / 'valid' / 'images')\n",
    "    data_yaml['test'] = str(city_dataset_dir / 'test' / 'images')\n",
    "\n",
    "    # Save new data.yaml\n",
    "    city_yaml_path = city_dataset_dir / 'data.yaml'\n",
    "    with open(city_yaml_path, 'w') as f:\n",
    "        yaml.dump(data_yaml, f)\n",
    "\n",
    "    print(f\"data.yaml for {city} saved.\\n\")\n",
    "\n",
    "print(\"All city datasets created successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db27ec4-3efc-4b4d-a968-4bc9d9f0dfd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cd382a0-59eb-48e7-b5af-9d4e30d5433b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”µ ISTANBUL test ediliyor...\n",
      "Ultralytics 8.3.91 ðŸš€ Python-3.10.15 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "                                                       CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "Model summary (fused): 112 layers, 43,608,921 parameters, 0 gradients, 164.8 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /truba/home/baakgul/roadtr-14032025-istanbul/valid/labels.cache... 259 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 259/259 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 594, len(boxes) = 4280. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 17/17 [00:07<00:00,  2.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        259       4280      0.672      0.591      0.567      0.415\n",
      "            pedestrian        168       1084      0.528      0.297       0.29      0.134\n",
      "                  road        259        269      0.952      0.955      0.962      0.826\n",
      "               vehicle        255       2927      0.535      0.521      0.449      0.284\n",
      "Speed: 0.7ms preprocess, 23.8ms inference, 0.0ms loss, 0.8ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolo-8-70-30-istanbul\u001b[0m\n",
      "âœ… ISTANBUL test tamamlandÄ±!\n",
      "\n",
      "ðŸ”µ PARIS test ediliyor...\n",
      "Ultralytics 8.3.91 ðŸš€ Python-3.10.15 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "                                                       CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /truba/home/baakgul/roadtr-14032025-paris/valid/labels.cache... 94 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 94/94 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 264, len(boxes) = 2732. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:03<00:00,  1.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         94       2732      0.778      0.649      0.674      0.465\n",
      "            pedestrian         81       1316      0.626      0.301      0.339      0.139\n",
      "                  road         94         97      0.962      0.938       0.95      0.827\n",
      "               vehicle         94       1319      0.747      0.709      0.732      0.429\n",
      "Speed: 1.6ms preprocess, 24.1ms inference, 0.0ms loss, 2.7ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolo-8-70-30-paris\u001b[0m\n",
      "âœ… PARIS test tamamlandÄ±!\n",
      "\n",
      "ðŸ”µ MUNIH test ediliyor...\n",
      "Ultralytics 8.3.91 ðŸš€ Python-3.10.15 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "                                                       CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /truba/home/baakgul/roadtr-14032025-munih/valid/labels.cache... 466 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 466/466 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mWARNING âš ï¸ /truba/home/baakgul/roadtr-14032025-munih/valid/images/munih-einstein-1551_jpg.rf.b106ab84b7efb048f2144b7c2f1a0e2c.jpg: 1 duplicate labels removed\n",
      "WARNING âš ï¸ Box and segment counts should be equal, but got len(segments) = 292, len(boxes) = 5319. To resolve this only boxes will be used and all segments will be removed. To avoid this please supply either a detect or segment dataset, not a detect-segment mixed dataset.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [00:13<00:00,  2.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        466       5319      0.742      0.677      0.706      0.451\n",
      "            pedestrian        168        894      0.628      0.431      0.462      0.193\n",
      "                  road        465        521      0.865      0.868      0.899      0.732\n",
      "               vehicle        444       3904      0.732      0.732      0.755      0.428\n",
      "Speed: 0.5ms preprocess, 23.7ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolo-8-70-30-munih\u001b[0m\n",
      "âœ… MUNIH test tamamlandÄ±!\n",
      "\n",
      "ðŸ”µ MARSILYA test ediliyor...\n",
      "Ultralytics 8.3.91 ðŸš€ Python-3.10.15 torch-2.6.0+cu124 CUDA:0 (Tesla P100-PCIE-16GB, 16269MiB)\n",
      "                                                       CUDA:1 (Tesla P100-PCIE-16GB, 16269MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /truba/home/baakgul/roadtr-14032025-marsilya/valid/labels.cache... 502 images, 1 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 502/502 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32/32 [00:14<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all        502       9146      0.814      0.688      0.718      0.475\n",
      "            pedestrian        267       2137        0.7      0.336      0.391       0.16\n",
      "                  road        485        495      0.955      0.976      0.964       0.83\n",
      "               vehicle        496       6514      0.789      0.753        0.8      0.436\n",
      "Speed: 0.5ms preprocess, 23.7ms inference, 0.0ms loss, 0.5ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/yolo-8-70-30-marsilya\u001b[0m\n",
      "âœ… MARSILYA test tamamlandÄ±!\n",
      "\n",
      "ðŸ TÃ¼m ÅŸehir class sonuÃ§larÄ± '/arf/home/baakgul/runs/detect/yolo-8-70-30/city_results/city_class_results.csv' dosyasÄ±na kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "README: YOLO City-wise Model Evaluation Collector\n",
    "\n",
    "This script:\n",
    "- Evaluates a trained YOLO model for each specified city (using city-specific data.yaml files),\n",
    "- Collects per-class metrics (Precision, Recall, mAP50, mAP50-95) for each city,\n",
    "- Saves results as a CSV file: [city_class_results.csv] under the results directory.\n",
    "\n",
    "How to use:\n",
    "- Set 'model_name', 'weights_path', and the dataset base path as needed.\n",
    "- Define your city list and folder conventions.\n",
    "- Run the script; a CSV with class metrics for each city will be created.\n",
    "\n",
    "Requirements:\n",
    "- ultralytics\n",
    "- torch\n",
    "- pandas\n",
    "\n",
    "Author: Bahadir Akin Akgul\n",
    "Date: 13.07.2025\n",
    "\"\"\"\n",
    "\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# === Configuration ===\n",
    "model_name = 'your-model-name-here'  # Change this to test different models\n",
    "weights_path = f'/PATH/TO/runs/detect/{model_name}/weights/best.pt'\n",
    "results_base = f'/PATH/TO/runs/detect/{model_name}/city_results'\n",
    "\n",
    "# === Load Model ===\n",
    "model = YOLO(weights_path)\n",
    "\n",
    "# === List of Cities ===\n",
    "cities = ['istanbul', 'paris', 'munich', 'marseille']\n",
    "\n",
    "# === Dataset base directory\n",
    "city_dataset_base = Path('/PATH/TO/dataset-root')\n",
    "\n",
    "# === Create results directory\n",
    "city_results_base = Path(results_base)\n",
    "city_results_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# === All results container\n",
    "all_results = []\n",
    "\n",
    "for city in cities:\n",
    "    print(f\"\\nTesting on: {city.upper()}\")\n",
    "\n",
    "    # Dataset/data.yaml path for the city\n",
    "    city_dataset = city_dataset_base / f'roadtr-YYYYMMDD-{city}'\n",
    "    city_data_yaml = city_dataset / 'data.yaml'\n",
    "\n",
    "    # Output directory for this city\n",
    "    city_save_dir = city_results_base / city\n",
    "    city_save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Validation\n",
    "    results = model.val(\n",
    "        data=str(city_data_yaml),\n",
    "        imgsz=1024,\n",
    "        batch=16,\n",
    "        device=[0, 1],         # Adjust to available GPUs\n",
    "        save_dir=str(city_save_dir),\n",
    "        save_json=False,\n",
    "        verbose=True,\n",
    "        plots=True,\n",
    "        conf=0.001,\n",
    "        rect=True,\n",
    "        name=f'{model_name}-{city}'\n",
    "    )\n",
    "\n",
    "    # Collect per-class metrics\n",
    "    names = results.names\n",
    "    p = results.box.p\n",
    "    r = results.box.r\n",
    "    ap50 = results.box.all_ap[:, 0]\n",
    "    ap = results.box.ap\n",
    "\n",
    "    n_classes = len(names)\n",
    "\n",
    "    for class_id in range(n_classes):\n",
    "        all_results.append({\n",
    "            'Model': model_name,\n",
    "            'City': city,\n",
    "            'Class': names[class_id],\n",
    "            'Precision': round(float(p[class_id]), 3),\n",
    "            'Recall': round(float(r[class_id]), 3),\n",
    "            'mAP50': round(float(ap50[class_id]), 3),\n",
    "            'mAP50-95': round(float(ap[class_id]), 3),\n",
    "        })\n",
    "\n",
    "    print(f\"Finished: {city.upper()}\")\n",
    "\n",
    "# Save as DataFrame and CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "csv_path = city_results_base / 'city_class_results.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nAll results saved to '{csv_path}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e7bc9d-f381-428c-96d2-922932a897fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

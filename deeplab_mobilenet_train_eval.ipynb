{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae1230ab-1aca-4bae-81d7-577b2f57484b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda, GPU count: 2\n",
      "2 GPU ile model paralelleştiriliyor...\n",
      "Checkpoint yüklendi. Eğitim 100. epoch'tan devam ediyor...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 56/56 [00:23<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Değerlendirme sonuçları kaydedildi.\n",
      "Model başarıyla kaydedildi!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "README: DeepLabV3-MobileNetV3 Semantic Segmentation - Training & Evaluation Pipeline\n",
    "\n",
    "This script provides a full pipeline for training and evaluating a semantic segmentation model\n",
    "(DeepLabV3 with MobileNetV3 backbone) on a custom dataset using PyTorch and Albumentations.\n",
    "\n",
    "Key Features:\n",
    "- Custom Dataset class to load image-mask pairs from specified folders.\n",
    "- Data augmentations and normalization using Albumentations.\n",
    "- Model: torchvision.models.segmentation.deeplabv3_mobilenet_v3_large (customizable class count).\n",
    "- Weighted CrossEntropyLoss for imbalanced datasets.\n",
    "- Training loop with checkpointing and persistent tracking of training loss (JSON + plot).\n",
    "- Evaluation with per-class metrics (IoU, Precision, Recall, F1) and saving results as JSON and PNG plots.\n",
    "- Multi-GPU support via DataParallel.\n",
    "\n",
    "How to use:\n",
    "1. Organize your data as follows:\n",
    "    YOUR_DATASET_DIR/\n",
    "        train/\n",
    "            image1.jpg\n",
    "            image1_mask.png\n",
    "            ...\n",
    "        valid/\n",
    "        test/\n",
    "2. Set the desired number of classes and class weights.\n",
    "3. Configure paths and hyperparameters as needed.\n",
    "4. Run the script to train and evaluate your segmentation model.\n",
    "\n",
    "Requirements:\n",
    "- torch\n",
    "- torchvision\n",
    "- albumentations\n",
    "- opencv-python\n",
    "- numpy\n",
    "- tqdm\n",
    "- matplotlib\n",
    "\n",
    "Author: Bahadir Akin Akgul\n",
    "Date: 13.07.2025\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Directory setup\n",
    "RESULTS_DIR = \"YOUR_RESULTS_DIR\"\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}, GPU count: {torch.cuda.device_count()}\")\n",
    "\n",
    "DATA_DIR = \"YOUR_DATASET_DIR\"\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, \"train\")\n",
    "VALID_DIR = os.path.join(DATA_DIR, \"valid\")\n",
    "TEST_DIR = os.path.join(DATA_DIR, \"test\")\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(1024, 768),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "], additional_targets={'mask': 'mask'})\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.images = [f for f in os.listdir(img_dir) if f.endswith(\".jpg\")]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.images[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        mask_path = img_path.replace(\".jpg\", \"_mask.png\")\n",
    "\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if mask is None or image.shape[:2] != mask.shape:\n",
    "            print(f\"Warning: {mask_path} could not be loaded! Image: {image.shape}, Mask: {mask.shape if mask is not None else 'None'}\")\n",
    "            mask = np.zeros((image.shape[0], image.shape[1]), dtype=np.uint8)\n",
    "        else:\n",
    "            mask = cv2.resize(mask, (image.shape[1], image.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"].long()\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "train_dataset = SegmentationDataset(TRAIN_DIR, transform=transform)\n",
    "valid_dataset = SegmentationDataset(VALID_DIR, transform=transform)\n",
    "test_dataset = SegmentationDataset(TEST_DIR, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True, num_workers=8, drop_last=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=24, shuffle=False, num_workers=8, drop_last=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False, num_workers=8, drop_last=False, pin_memory=True)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "model = torchvision.models.segmentation.deeplabv3_mobilenet_v3_large(weights=None)\n",
    "model.classifier[4] = torch.nn.Conv2d(256, NUM_CLASSES, kernel_size=1)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Parallelizing model on {torch.cuda.device_count()} GPUs...\")\n",
    "    model = torch.nn.DataParallel(model)\n",
    "\n",
    "class_weights = torch.tensor([0.2, 1.0, 1.0, 1.0]).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def load_checkpoint(model, optimizer, checkpoint_path=os.path.join(RESULTS_DIR, \"checkpoint.pth\")):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "        model_to_load = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "        model_to_load.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        start_epoch = checkpoint[\"epoch\"]\n",
    "        print(f\"Checkpoint loaded. Continuing from epoch {start_epoch}.\")\n",
    "        return model, optimizer, start_epoch\n",
    "    else:\n",
    "        print(\"No checkpoint found. Starting training from scratch.\")\n",
    "        return model, optimizer, 0\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, optimizer, criterion, start_epoch=0, epochs=100, checkpoint_path=os.path.join(RESULTS_DIR, \"checkpoint.pth\")):\n",
    "    # Load train_losses if available\n",
    "    train_losses = []\n",
    "    loss_file = os.path.join(RESULTS_DIR, \"train_losses.json\")\n",
    "    if os.path.exists(loss_file):\n",
    "        with open(loss_file, \"r\") as f:\n",
    "            train_losses = json.load(f)\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, masks in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)[\"out\"]\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Train Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # Save checkpoint\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"model_state_dict\": model.module.state_dict() if isinstance(model, torch.nn.DataParallel) else model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "        print(f\"Checkpoint saved: {checkpoint_path}\")\n",
    "\n",
    "        # Save train_losses\n",
    "        with open(loss_file, \"w\") as f:\n",
    "            json.dump(train_losses, f)\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, 1+len(train_losses)), train_losses, marker='o')\n",
    "    plt.title(\"Train Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid()\n",
    "    plt.savefig(os.path.join(RESULTS_DIR, \"train_loss.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, dataloader, num_classes=NUM_CLASSES):\n",
    "    model.eval()\n",
    "    iou_per_class = [0.0] * num_classes\n",
    "    precision_per_class = [0.0] * num_classes\n",
    "    recall_per_class = [0.0] * num_classes\n",
    "    f1_per_class = [0.0] * num_classes\n",
    "    total_per_class = [0] * num_classes\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            images, masks = images.to(DEVICE), masks.to(DEVICE)\n",
    "            outputs = model(images)[\"out\"]\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for cls in range(num_classes):\n",
    "                pred_cls = (preds == cls).int()\n",
    "                true_cls = (masks == cls).int()\n",
    "\n",
    "                TP = (pred_cls & true_cls).sum().item()\n",
    "                FP = (pred_cls & (1 - true_cls)).sum().item()\n",
    "                FN = ((1 - pred_cls) & true_cls).sum().item()\n",
    "\n",
    "                if TP + FP + FN == 0:\n",
    "                    continue  # This class does not appear in this batch\n",
    "\n",
    "                precision = TP / (TP + FP + 1e-8)\n",
    "                recall = TP / (TP + FN + 1e-8)\n",
    "                f1 = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "                iou = TP / (TP + FP + FN + 1e-8)\n",
    "\n",
    "                precision_per_class[cls] += precision\n",
    "                recall_per_class[cls] += recall\n",
    "                f1_per_class[cls] += f1\n",
    "                iou_per_class[cls] += iou\n",
    "                total_per_class[cls] += 1\n",
    "\n",
    "    results = {\n",
    "        \"precision\": [p / max(n, 1) if n > 0 else None for p, n in zip(precision_per_class, total_per_class)],\n",
    "        \"recall\": [r / max(n, 1) if n > 0 else None for r, n in zip(recall_per_class, total_per_class)],\n",
    "        \"f1\": [f / max(n, 1) if n > 0 else None for f, n in zip(f1_per_class, total_per_class)],\n",
    "        \"iou\": [i / max(n, 1) if n > 0 else None for i, n in zip(iou_per_class, total_per_class)],\n",
    "    }\n",
    "\n",
    "    with open(os.path.join(RESULTS_DIR, \"metrics_epoch.json\"), \"w\") as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "\n",
    "    # Plot metrics for classes present in the set\n",
    "    classes = [i for i, n in enumerate(total_per_class) if n > 0]\n",
    "    for metric_name in [\"precision\", \"recall\", \"f1\", \"iou\"]:\n",
    "        plt.figure()\n",
    "        values = [results[metric_name][i] for i in classes]\n",
    "        plt.bar([str(c) for c in classes], values)\n",
    "        plt.title(f\"{metric_name.upper()} per Class\")\n",
    "        plt.xlabel(\"Class\")\n",
    "        plt.ylabel(metric_name.upper())\n",
    "        plt.ylim(0, 1.05)\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(RESULTS_DIR, f\"{metric_name}_per_class.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    print(\"Evaluation results saved.\")\n",
    "\n",
    "model, optimizer, start_epoch = load_checkpoint(model, optimizer)\n",
    "train_model(model, train_loader, valid_loader, optimizer, criterion, start_epoch=start_epoch, epochs=100)\n",
    "evaluate_model(model, valid_loader)\n",
    "\n",
    "model_to_save = model.module if isinstance(model, torch.nn.DataParallel) else model\n",
    "torch.save(model_to_save.state_dict(), os.path.join(RESULTS_DIR, \"trained_model.pth\"))\n",
    "print(\"Model saved successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ce5409-fd4b-4b19-bb6e-7cf74990b35e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
